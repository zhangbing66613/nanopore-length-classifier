"""
åŸºäºç¥ç»ç½‘ç»œçš„çº³ç±³å­”Lengthåˆ†ç±»ç³»ç»Ÿ
ä½¿ç”¨ Dwell Time(s), Amplitude(pA), ECD(pC), size å››ä¸ªå‚æ•°è¿›è¡ŒLengthåˆ†ç±»
ç‰ˆæœ¬ï¼š2.2 (å·²é›†æˆå­”å†…ç‰¹å¾å¯¹æ¯”åˆ†æ)
"""

import os
import sys
import pandas as pd
import numpy as np
from pathlib import Path
import warnings

warnings.filterwarnings('ignore')

# ============ æ£€æŸ¥å¹¶å®‰è£…å¿…è¦çš„åº“ ============
REQUIRED_LIBRARIES = [
    ('tensorflow', 'tensorflow'),
    ('scipy', 'scipy'),
    ('sklearn', 'scikit-learn'),
    ('seaborn', 'seaborn'),
    ('joblib', 'joblib'),
    ('openpyxl', 'openpyxl')
]

print("=" * 80)
print("æ£€æŸ¥ä¾èµ–åº“...")
print("=" * 80)

missing_libs = []
for import_name, pip_name in REQUIRED_LIBRARIES:
    try:
        __import__(import_name)
        print(f"âœ… {import_name}")
    except ImportError:
        print(f"âŒ {import_name}")
        missing_libs.append(pip_name)

if missing_libs:
    print(f"\nç¼ºå°‘ä»¥ä¸‹åº“: {', '.join(missing_libs)}")
    install = input("æ˜¯å¦è‡ªåŠ¨å®‰è£…? (y/n) [é»˜è®¤: y]: ").strip().lower()
    if install != 'n':
        import subprocess

        for lib in missing_libs:
            print(f"æ­£åœ¨å®‰è£… {lib}...")
            subprocess.check_call([sys.executable, "-m", "pip", "install", lib])
        print("âœ… æ‰€æœ‰ä¾èµ–åº“å®‰è£…å®Œæˆ!")
    else:
        print("è¯·æ‰‹åŠ¨å®‰è£…ç¼ºå°‘çš„åº“åé‡æ–°è¿è¡Œç¨‹åºã€‚")
        sys.exit(1)

# ============ å¯¼å…¥åº“ ============
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models, regularizers
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
from scipy import stats
# ============ ä¸­æ–‡æ˜¾ç¤ºè®¾ç½® ============
# è®¾ç½®ä¸­æ–‡å­—ä½“ï¼ˆè§£å†³ä¸­æ–‡æ˜¾ç¤ºä¸ºæ–¹æ¡†é—®é¢˜ï¼‰
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']  # ä¸­æ–‡æ˜¾ç¤º
plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
# ============ ä¸­æ–‡è®¾ç½®ç»“æŸ ============
# è®¾ç½®éšæœºç§å­
np.random.seed(42)
tf.random.set_seed(42)


# ============ ä¸»ç±»å®šä¹‰ ============
class NanoPoreLengthClassifier:
    """çº³ç±³å­”Lengthåˆ†ç±»å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–åˆ†ç±»å™¨"""
        self.data = None
        self.X = None
        self.y = None
        self.feature_names = ['Dwell Time (s)', 'Amplitude (pA)', 'ECD (pC)', 'size']
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()
        self.model = None
        self.history = None
        self.config = {}

    # ============ æ ¸å¿ƒæ–¹æ³• ============

    def run_interactive_setup(self):
        """äº¤äº’å¼å‚æ•°è®¾ç½®"""
        print("\n" + "=" * 80)
        print("ç¥ç»ç½‘ç»œLengthåˆ†ç±»ç³»ç»Ÿ - äº¤äº’å¼è®¾ç½®")
        print("=" * 80)

        # 1. æ•°æ®è·¯å¾„
        print("\n[1/7] æ•°æ®æ–‡ä»¶å¤¹è®¾ç½®")
        default_folder = r"D:\ecd\analysis\çº³ç±³å­”æ•°æ®"
        while True:
            folder = input(f"æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„ [é»˜è®¤: {default_folder}]: ").strip()
            folder = folder if folder else default_folder

            if os.path.exists(folder):
                self.config['data_folder'] = folder
                print(f"âœ… æ•°æ®æ–‡ä»¶å¤¹: {folder}")
                break
            else:
                print(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {folder}")
                if input("é‡è¯•? (y/n) [y]: ").strip().lower() == 'n':
                    sys.exit(0)

        # 2. åˆ—åé…ç½®
        print("\n[2/7] åˆ—åé…ç½®")
        self.config['pore_column'] = input("Pore Numberåˆ—å [é»˜è®¤: 'Pore Number']: ").strip() or "Pore Number"
        print(f"âœ… Pore Numberåˆ—: {self.config['pore_column']}")

        # 3. Lengthé€‰æ‹©
        print("\n[3/7] Lengthé€‰æ‹©")
        print("1. è‡ªåŠ¨é€‰æ‹©æœ€å¸¸è§Length")
        print("2. æ‰‹åŠ¨æŒ‡å®šLength")
        choice = input("é€‰æ‹©æ–¹å¼ (1/2) [é»˜è®¤: 1]: ").strip() or "1"

        if choice == "1":
            self.config['target_lengths'] = None
            min_samples = input("æ¯ä¸ªç±»åˆ«æœ€å°‘æ ·æœ¬æ•° [é»˜è®¤: 100]: ").strip()
            self.config['min_samples'] = int(min_samples) if min_samples else 100
            print(f"âœ… è‡ªåŠ¨é€‰æ‹© (æœ€å°‘{self.config['min_samples']}æ ·æœ¬)")
        else:
            lengths = input("è¾“å…¥Lengthå€¼ï¼Œç”¨é€—å·åˆ†éš” (å¦‚: 100,200,300,400,500): ").strip()
            if lengths:
                self.config['target_lengths'] = [int(x.strip()) for x in lengths.split(',')]
                print(f"âœ… æ‰‹åŠ¨æŒ‡å®š: {self.config['target_lengths']}")
            else:
                self.config['target_lengths'] = None
                print("âš  ä½¿ç”¨è‡ªåŠ¨é€‰æ‹©")

        # 4. æ•°æ®åˆ†å‰²
        print("\n[4/7] æ•°æ®åˆ†å‰²")
        test_size = input("æµ‹è¯•é›†æ¯”ä¾‹ (0.1-0.4) [é»˜è®¤: 0.2]: ").strip()
        self.config['test_size'] = float(test_size) if test_size else 0.2
        print(f"âœ… æµ‹è¯•é›†æ¯”ä¾‹: {self.config['test_size']}")

        # 5. ç¥ç»ç½‘ç»œç»“æ„
        print("\n[5/7] ç¥ç»ç½‘ç»œç»“æ„")
        print("1. ç®€å• (32-16)")
        print("2. ä¸­ç­‰ (64-32-16) [æ¨è]")
        print("3. å¤æ‚ (128-64-32-16)")
        print("4. è‡ªå®šä¹‰")

        nn_choice = input("é€‰æ‹©ç»“æ„ (1/2/3/4) [é»˜è®¤: 2]: ").strip() or "2"

        if nn_choice == "1":
            self.config['hidden_layers'] = [32, 16]
        elif nn_choice == "2":
            self.config['hidden_layers'] = [64, 32, 16]
        elif nn_choice == "3":
            self.config['hidden_layers'] = [128, 64, 32, 16]
        else:
            custom = input("è¾“å…¥éšè—å±‚ç¥ç»å…ƒæ•°ï¼Œé€—å·åˆ†éš” (å¦‚: 128,64,32): ").strip()
            self.config['hidden_layers'] = [int(x) for x in custom.split(',')] if custom else [64, 32, 16]

        print(f"âœ… ç½‘ç»œç»“æ„: {self.config['hidden_layers']}")

        # 6. è®­ç»ƒå‚æ•°
        print("\n[6/7] è®­ç»ƒå‚æ•°")
        self.config['epochs'] = int(input("è®­ç»ƒè½®æ•° [é»˜è®¤: 100]: ").strip() or 100)
        self.config['batch_size'] = int(input("æ‰¹æ¬¡å¤§å° [é»˜è®¤: 32]: ").strip() or 32)
        self.config['learning_rate'] = float(input("å­¦ä¹ ç‡ [é»˜è®¤: 0.001]: ").strip() or 0.001)
        self.config['dropout_rate'] = float(input("Dropoutç‡ [é»˜è®¤: 0.3]: ").strip() or 0.3)

        print(f"âœ… è®­ç»ƒè½®æ•°: {self.config['epochs']}, æ‰¹æ¬¡: {self.config['batch_size']}")
        print(f"âœ… å­¦ä¹ ç‡: {self.config['learning_rate']}, Dropout: {self.config['dropout_rate']}")

        # 7. è¾“å‡ºè®¾ç½®
        print("\n[7/7] è¾“å‡ºè®¾ç½®")
        default_output = os.path.join(os.path.dirname(self.config['data_folder']), "length_classification_results")
        output = input(f"è¾“å‡ºæ–‡ä»¶å¤¹ [é»˜è®¤: {default_output}]: ").strip() or default_output
        self.config['output_folder'] = output
        os.makedirs(output, exist_ok=True)
        print(f"âœ… è¾“å‡ºæ–‡ä»¶å¤¹: {output}")

        # æ˜¾ç¤ºé…ç½®æ‘˜è¦
        self._show_config_summary()

        return self.config

    def _show_config_summary(self):
        """æ˜¾ç¤ºé…ç½®æ‘˜è¦"""
        print("\n" + "=" * 80)
        print("é…ç½®æ‘˜è¦")
        print("=" * 80)

        summary = f"""
        æ•°æ®æ–‡ä»¶å¤¹: {self.config['data_folder']}
        Pore Numberåˆ—: {self.config['pore_column']}
        Lengthé€‰æ‹©: {'è‡ªåŠ¨é€‰æ‹©' if self.config['target_lengths'] is None else f'æ‰‹åŠ¨æŒ‡å®š: {self.config["target_lengths"]}'}
        æµ‹è¯•é›†æ¯”ä¾‹: {self.config['test_size']}
        ç½‘ç»œç»“æ„: {self.config['hidden_layers']}
        è®­ç»ƒå‚æ•°: {self.config['epochs']}è½®, æ‰¹æ¬¡{self.config['batch_size']}
        å­¦ä¹ ç‡: {self.config['learning_rate']}, Dropout: {self.config['dropout_rate']}
        è¾“å‡ºæ–‡ä»¶å¤¹: {self.config['output_folder']}
        """
        print(summary)

        confirm = input("\né…ç½®æ˜¯å¦æ­£ç¡®? å¼€å§‹æ‰§è¡Œ? (y/n) [y]: ").strip().lower()
        if confirm == 'n':
            print("é‡æ–°é…ç½®...")
            self.run_interactive_setup()

    def load_and_preprocess_data(self):
        """åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®ï¼ˆæ”¯æŒåˆ—åå¤§å°å†™æ¨¡ç³ŠåŒ¹é…ï¼‰"""
        print("\n" + "=" * 80)
        print("æ•°æ®åŠ è½½ä¸é¢„å¤„ç†")
        print("=" * 80)

        data_folder = self.config['data_folder']
        excel_exts = ('.xlsx', '.xls', '.xlsm', '.xlsb')
        all_data = []
        problematic_files = []

        print(f"æ‰«ææ–‡ä»¶å¤¹: {data_folder}")

        # å®šä¹‰å¿…éœ€åˆ—ï¼ˆæ ‡å‡†åç§°ï¼‰
        required_columns = self.feature_names + ['Length', self.config['pore_column']]

        for root, _, files in os.walk(data_folder):
            for file in files:
                if file.lower().endswith(excel_exts):
                    try:
                        file_path = os.path.join(root, file)
                        df = pd.read_excel(file_path)

                        # åˆ—åæ ‡å‡†åŒ–ï¼šä¸åŒºåˆ†å¤§å°å†™åŒ¹é…
                        column_mapping = {}
                        missing_cols = []

                        for std_col in required_columns:
                            # æŸ¥æ‰¾åŒ¹é…çš„åˆ—ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
                            matches = [col for col in df.columns
                                       if str(col).strip().lower() == std_col.lower()]

                            if matches:
                                actual_col = matches[0]
                                if actual_col != std_col:
                                    column_mapping[actual_col] = std_col
                            else:
                                missing_cols.append(std_col)

                        if missing_cols:
                            problematic_files.append((file, f"ç¼ºå°‘åˆ—: {missing_cols}"))
                            continue

                        # é‡å‘½ååˆ—
                        if column_mapping:
                            df = df.rename(columns=column_mapping)

                        # æ·»åŠ æ–‡ä»¶ä¿¡æ¯
                        df['Source_File'] = file

                        # å°è¯•æŸ¥æ‰¾EventIDåˆ—ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
                        event_id_candidates = ['EventID', 'eventid', 'Event_ID', 'event_id', 'Id', 'ID', 'Index']
                        for candidate in event_id_candidates:
                            matches = [col for col in df.columns
                                       if str(col).strip().lower() == candidate.lower()]
                            if matches:
                                df['EventID'] = df[matches[0]]
                                break

                        all_data.append(df)

                    except Exception as e:
                        problematic_files.append((file, f"è¯»å–é”™è¯¯: {str(e)[:100]}"))

        # åˆå¹¶æ•°æ®
        if not all_data:
            raise ValueError("âŒ æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆæ•°æ®æ–‡ä»¶ï¼")

        self.data = pd.concat(all_data, ignore_index=True)

        print(f"\nâœ… æ•°æ®åŠ è½½å®Œæˆ")
        print(f"   æˆåŠŸåŠ è½½æ–‡ä»¶: {len(all_data)}ä¸ª")
        print(f"   æ€»æ•°æ®è¡Œæ•°: {len(self.data):,}")

        if problematic_files:
            print(f"\nâš  æœ‰é—®é¢˜çš„æ–‡ä»¶ ({len(problematic_files)}ä¸ª):")
            for file, reason in problematic_files[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                print(f"   - {file}: {reason}")
            if len(problematic_files) > 10:
                print(f"   ... è¿˜æœ‰{len(problematic_files) - 10}ä¸ªæ–‡ä»¶")

        # æ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯
        print(f"\nğŸ“Š æ•°æ®åŸºæœ¬ä¿¡æ¯:")
        print(f"   åˆ—: {list(self.data.columns)}")
        print(f"   å”¯ä¸€Lengthå€¼: {self.data['Length'].nunique()}ä¸ª")

        # Lengthåˆ†å¸ƒ
        length_counts = self.data['Length'].value_counts()
        print(f"\nğŸ“ˆ Lengthåˆ†å¸ƒ (å‰10):")
        for length, count in length_counts.head(10).items():
            print(f"   Length {length}: {count:,}è¡Œ ({count / len(self.data) * 100:.1f}%)")

        return self.data

    def select_target_lengths(self):
        """é€‰æ‹©ç›®æ ‡Length"""
        print("\n" + "=" * 80)
        print("Lengthé€‰æ‹©")
        print("=" * 80)

        length_counts = self.data['Length'].value_counts()

        if self.config['target_lengths'] is None:
            # è‡ªåŠ¨é€‰æ‹©
            min_samples = self.config.get('min_samples', 100)
            common_lengths = length_counts[length_counts >= min_samples].index.tolist()

            if len(common_lengths) < 2:
                print(f"âš  æ•°æ®é‡ä¸è¶³ï¼Œé™ä½è¦æ±‚...")
                common_lengths = length_counts.head(5).index.tolist()

            self.config['target_lengths'] = common_lengths
            print(f"âœ… è‡ªåŠ¨é€‰æ‹© {len(common_lengths)} ä¸ªLength: {common_lengths}")
        else:
            # æ£€æŸ¥æ‰‹åŠ¨æŒ‡å®šçš„Lengthæ˜¯å¦å­˜åœ¨
            available = set(self.data['Length'].unique())
            specified = set(self.config['target_lengths'])
            missing = specified - available

            if missing:
                print(f"âš  ä»¥ä¸‹Lengthä¸å­˜åœ¨: {missing}")
                self.config['target_lengths'] = list(specified & available)
                print(f"âœ… ä½¿ç”¨å­˜åœ¨çš„Length: {self.config['target_lengths']}")

        # ç­›é€‰æ•°æ®
        before = len(self.data)
        self.data = self.data[self.data['Length'].isin(self.config['target_lengths'])].copy()
        after = len(self.data)

        print(f"\nâœ… æ•°æ®ç­›é€‰å®Œæˆ")
        print(f"   ç­›é€‰å‰: {before:,}è¡Œ")
        print(f"   ç­›é€‰å: {after:,}è¡Œ (ä¿ç•™{after / before * 100:.1f}%)")

        # æ˜¾ç¤ºæœ€ç»ˆåˆ†å¸ƒ
        print(f"\nğŸ“Š æœ€ç»ˆLengthåˆ†å¸ƒ:")
        final_counts = self.data['Length'].value_counts()
        for length, count in final_counts.items():
            print(f"   Length {length}: {count:,}è¡Œ ({count / after * 100:.1f}%)")

        return self.data

    def explore_data(self):
        """æ¢ç´¢æ€§æ•°æ®åˆ†æ"""
        print("\n" + "=" * 80)
        print("æ¢ç´¢æ€§æ•°æ®åˆ†æ")
        print("=" * 80)

        output_dir = os.path.join(self.config['output_folder'], "exploration")
        os.makedirs(output_dir, exist_ok=True)

        # 1. åŸºæœ¬ç»Ÿè®¡
        print("\nğŸ“‹ ç‰¹å¾ç»Ÿè®¡:")
        stats_df = self.data[self.feature_names].describe().round(4)
        print(stats_df)
        stats_df.to_csv(os.path.join(output_dir, "basic_statistics.csv"))

        # 2. ç›¸å…³æ€§åˆ†æ
        print("\nğŸ”— ç‰¹å¾ç›¸å…³æ€§:")
        corr_matrix = self.data[self.feature_names].corr().round(3)
        print(corr_matrix)
        corr_matrix.to_csv(os.path.join(output_dir, "correlation_matrix.csv"))

        # å¯è§†åŒ–ç›¸å…³æ€§
        plt.figure(figsize=(8, 6))
        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, square=True)
        plt.title('ç‰¹å¾ç›¸å…³æ€§çƒ­å›¾')
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'correlation_heatmap.png'), dpi=300)
        plt.show(block=False)
        plt.pause(2)
        plt.close()

        # 3. Lengthåˆ†å¸ƒå›¾
        plt.figure(figsize=(10, 6))
        length_counts = self.data['Length'].value_counts().sort_index()
        plt.bar(range(len(length_counts)), length_counts.values)
        plt.xticks(range(len(length_counts)), [str(x) for x in length_counts.index], rotation=45)
        plt.xlabel('Length')
        plt.ylabel('æ ·æœ¬æ•°é‡')
        plt.title('Lengthåˆ†å¸ƒ')
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'length_distribution.png'), dpi=300)
        plt.show(block=False)
        plt.pause(2)
        plt.close()

        print(f"\nâœ… æ¢ç´¢æ€§åˆ†æå®Œæˆ")
        print(f"   ç»“æœä¿å­˜åˆ°: {output_dir}")

        return True

    def analyze_ecd_outliers(self):
        """åˆ†æECDå¼‚å¸¸å€¼ï¼ˆåŒ…å«è¯¦ç»†æ¥æºï¼‰"""
        print("\n" + "=" * 80)
        print("ECDå¼‚å¸¸å€¼åˆ†æ")
        print("=" * 80)

        ecd_series = self.data['ECD (pC)']

        print("ğŸ“Š ECDç»Ÿè®¡æ‘˜è¦:")
        print(f"   ä¸­ä½æ•°: {ecd_series.median():.2f} pC")
        print(f"   å‡å€¼:   {ecd_series.mean():.2f} pC")
        print(f"   æ ‡å‡†å·®: {ecd_series.std():.2f} pC")
        print(f"   æœ€å°å€¼: {ecd_series.min():.2f} pC")
        print(f"   æœ€å¤§å€¼: {ecd_series.max():.2f} pC")
        print(f"   99%åˆ†ä½æ•°: {ecd_series.quantile(0.99):.2f} pC")

        # ä½¿ç”¨IQRæ–¹æ³•è¯†åˆ«å¼‚å¸¸å€¼
        Q1 = ecd_series.quantile(0.25)
        Q3 = ecd_series.quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        outliers_mask = (ecd_series < lower_bound) | (ecd_series > upper_bound)
        outliers_count = outliers_mask.sum()

        print(f"\nğŸ” å¼‚å¸¸å€¼æ£€æµ‹ (IQRæ–¹æ³•):")
        print(f"   Q1 (25%): {Q1:.2f} pC")
        print(f"   Q3 (75%): {Q3:.2f} pC")
        print(f"   IQR: {IQR:.2f} pC")
        print(f"   æ­£å¸¸èŒƒå›´: [{lower_bound:.2f}, {upper_bound:.2f}] pC")
        print(f"   å¼‚å¸¸å€¼æ•°é‡: {outliers_count:,} ({outliers_count / len(ecd_series) * 100:.2f}%)")

        if outliers_count > 0:
            # åˆ›å»ºè¯¦ç»†æŠ¥å‘Š
            outliers_data = self.data[outliers_mask].copy()

            # ç¡®ä¿æœ‰EventIDåˆ—
            if 'EventID' not in outliers_data.columns:
                outliers_data['EventID'] = outliers_data.index

            # é€‰æ‹©è¦è¾“å‡ºçš„åˆ—
            output_cols = ['Source_File', 'EventID', 'ECD (pC)', 'Length']
            for col in self.feature_names:
                if col != 'ECD (pC)' and col in outliers_data.columns:
                    output_cols.append(col)

            outliers_report = outliers_data[output_cols].copy()
            outliers_report = outliers_report.sort_values('ECD (pC)', ascending=False)

            # ä¿å­˜æŠ¥å‘Š
            output_dir = os.path.join(self.config['output_folder'], "outlier_analysis")
            os.makedirs(output_dir, exist_ok=True)

            report_path = os.path.join(output_dir, "ecd_outliers_detailed.csv")
            outliers_report.to_csv(report_path, index=False, encoding='utf-8-sig')

            print(f"\nğŸ“‹ å¼‚å¸¸å€¼è¯¦ç»†æŠ¥å‘Š:")
            print(f"   å¼‚å¸¸å€¼æ€»æ•°: {outliers_count}")
            print(f"   æŒ‰æ–‡ä»¶åˆ†å¸ƒ:")
            file_dist = outliers_data['Source_File'].value_counts()
            for file, count in file_dist.head(10).items():
                print(f"     - {file}: {count}ä¸ª")

            print(f"\nğŸ“„ å‰10ä¸ªæœ€ä¸¥é‡çš„å¼‚å¸¸å€¼:")
            print(outliers_report.head(10).to_string(index=False))

            print(f"\nğŸ’¾ å®Œæ•´æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

            # è¯¢é—®å¦‚ä½•å¤„ç†
            print("\n" + "-" * 40)
            print("å¼‚å¸¸å€¼å¤„ç†é€‰é¡¹:")
            print("1. ç§»é™¤æ‰€æœ‰å¼‚å¸¸å€¼")
            print("2. ä»…ç§»é™¤ECD > 99%åˆ†ä½æ•°çš„æç«¯å€¼")
            print("3. ä¿ç•™æ‰€æœ‰æ•°æ®ï¼ˆä¸å¤„ç†ï¼‰")
            print("4. æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Šåæ‰‹åŠ¨å¤„ç†")

            choice = input("\nè¯·é€‰æ‹©å¤„ç†æ–¹å¼ (1/2/3/4) [é»˜è®¤: 2]: ").strip() or "2"

            if choice == "1":
                # ç§»é™¤æ‰€æœ‰IQRå¼‚å¸¸å€¼
                clean_data = self.data[~outliers_mask].copy()
                removed = len(self.data) - len(clean_data)
                self.data = clean_data
                print(f"âœ… å·²ç§»é™¤æ‰€æœ‰å¼‚å¸¸å€¼: {removed:,}è¡Œ")

            elif choice == "2":
                # ç§»é™¤99%åˆ†ä½æ•°ä»¥ä¸Šçš„æç«¯å€¼
                threshold = ecd_series.quantile(0.99)
                extreme_mask = ecd_series > threshold
                clean_data = self.data[~extreme_mask].copy()
                removed = extreme_mask.sum()
                self.data = clean_data
                print(f"âœ… å·²ç§»é™¤ECD > {threshold:.2f} pCçš„æç«¯å€¼: {removed:,}è¡Œ")

            elif choice == "4":
                print(f"\nğŸ“‹ è¯·æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š: {report_path}")
                print("æ‚¨å¯ä»¥åœ¨Excelä¸­æ‰“å¼€CSVæ–‡ä»¶æŸ¥çœ‹æ‰€æœ‰å¼‚å¸¸å€¼")
                input("æŒ‰Enteré”®ç»§ç»­...")

        return outliers_count if 'outliers_count' in locals() else 0

    def prepare_training_data(self):
        """å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆå·²åŠ å…¥æ•°æ®æ¸…æ´—ï¼‰"""
        print("\n" + "=" * 80)
        print("å‡†å¤‡è®­ç»ƒæ•°æ®")
        print("=" * 80)
        print("\nğŸ”§ åŸºäºå­”å†…åˆ†æåˆ›å»ºæ–°çš„ç›¸å¯¹ç‰¹å¾...")

        # ç¡®ä¿æœ‰Pore Numberåˆ—
        pore_column = self.config.get('pore_column', 'Pore Number')

        if pore_column in self.data.columns:
            # ä¸ºæ¯ä¸ªå­”è®¡ç®—400bpçš„å¹³å‡ç‰¹å¾å€¼ä½œä¸ºå‚è€ƒ
            pore_400_means = self.data[self.data['Length'] == 400].groupby(pore_column)[self.feature_names].mean()
            pore_400_means = pore_400_means.rename(columns={col: f'{col}_400_ref' for col in self.feature_names})

            # å°†å‚è€ƒå€¼åˆå¹¶åˆ°åŸå§‹æ•°æ®
            self.data = self.data.merge(pore_400_means, how='left', left_on=pore_column, right_index=True)

            # åˆ›å»ºç›¸å¯¹ç‰¹å¾ï¼ˆå½“å‰äº‹ä»¶ / åŒå­”400bpå¹³å‡å€¼ï¼‰
            for feature in self.feature_names:
                ref_col = f'{feature}_400_ref'
                if ref_col in self.data.columns:
                    # é¿å…é™¤é›¶
                    valid_mask = self.data[ref_col] != 0
                    self.data.loc[valid_mask, f'{feature}_ratio_to_400'] = self.data.loc[valid_mask, feature] / \
                                                                           self.data.loc[valid_mask, ref_col]

            # æ›´æ–°ç‰¹å¾åç§°åˆ—è¡¨
            new_features = [f'{feature}_ratio_to_400' for feature in self.feature_names]
            self.feature_names.extend(new_features)

            print(f"âœ… å·²åˆ›å»º {len(new_features)} ä¸ªæ–°çš„ç›¸å¯¹ç‰¹å¾:")
            # ã€åœ¨ç¬¬507è¡Œä¹‹åæ·»åŠ ä»¥ä¸‹ä»£ç ã€‘
            # 3. åˆ›å»ºç‰©ç†æ„ä¹‰çš„å¤åˆç‰¹å¾
            print(f"\nğŸ”§ åˆ›å»ºç‰©ç†æ„ä¹‰çš„å¤åˆç‰¹å¾...")

            # Dwell Timeå’ŒECDéƒ½æ˜¯é•¿åº¦æ•æ„Ÿçš„ï¼Œå®ƒä»¬çš„æ¯”å€¼å¯èƒ½æ›´ç¨³å®š
            if 'ECD (pC)' in self.data.columns and 'Dwell Time (s)' in self.data.columns:
                valid_mask = self.data['Dwell Time (s)'] != 0
                self.data.loc[valid_mask, 'ECD_per_Dwell'] = self.data.loc[valid_mask, 'ECD (pC)'] / self.data.loc[
                    valid_mask, 'Dwell Time (s)']
                self.feature_names.append('ECD_per_Dwell')
                print(f"   - ECD_per_Dwell: ç”µè·è½¬ç§»é€Ÿç‡(ECD/Dwell Time)")

            # Amplitudeä¸sizeçš„æ¯”å€¼å¯èƒ½åæ˜ å­”å¾„å¤§å°çš„å½±å“
            if 'Amplitude (pA)' in self.data.columns and 'size' in self.data.columns:
                valid_mask = self.data['size'] != 0
                self.data.loc[valid_mask, 'Amp_per_size'] = self.data.loc[valid_mask, 'Amplitude (pA)'] / self.data.loc[
                    valid_mask, 'size']
                self.feature_names.append('Amp_per_size')
                print(f"   - Amp_per_size: å•ä½sizeçš„ç”µæµå¹…åº¦")

            # åˆ›å»ºå¯¹æ•°å˜æ¢ç‰¹å¾ï¼ˆå¯¹äºåæ€åˆ†å¸ƒå¯èƒ½æ›´æœ‰æ•ˆï¼‰
            for col in ['Dwell Time (s)', 'ECD (pC)', 'Amplitude (pA)']:
                if col in self.data.columns:
                    # ç¡®ä¿æ‰€æœ‰å€¼ä¸ºæ­£
                    positive_mask = self.data[col] > 0
                    if positive_mask.any():
                        log_col = f'log_{col.replace(" (s)", "").replace(" (pA)", "").replace(" (pC)", "")}'
                        self.data.loc[positive_mask, log_col] = np.log(self.data.loc[positive_mask, col])
                        # å¯¹è´Ÿå€¼æˆ–é›¶ç”¨æœ€å°å€¼å¡«å……
                        if not positive_mask.all():
                            min_log = self.data.loc[positive_mask, log_col].min()
                            self.data.loc[~positive_mask, log_col] = min_log
                        self.feature_names.append(log_col)
                        print(f"   - {log_col}: {col}çš„å¯¹æ•°å˜æ¢")

        else:
            print("âš  æœªæ‰¾åˆ°Pore Numberåˆ—ï¼Œè·³è¿‡ç›¸å¯¹ç‰¹å¾åˆ›å»º")
        # ============ æ–°å¢ç‰¹å¾ç»“æŸ ============

        # æå–ç‰¹å¾å’Œæ ‡ç­¾ï¼ˆç°åœ¨åŒ…å«æ›´å¤šç‰¹å¾ï¼‰
        X = self.data[self.feature_names].values
        y = self.data['Length'].values

        # æå–ç‰¹å¾å’Œæ ‡ç­¾
        X = self.data[self.feature_names].values
        y = self.data['Length'].values

        # ============ æ•°æ®æ¸…æ´—ï¼šå¤„ç†NaNå’Œæ— ç©·å¤§å€¼ ============
        print("\nğŸ”§ æ•°æ®æ¸…æ´—ï¼šå¤„ç†ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼...")
        X = pd.DataFrame(X, columns=self.feature_names)  # å…ˆè½¬ä¸ºDataFrameä¾¿äºå¤„ç†

        for col in self.feature_names:
            # å¤„ç†æ— ç©·å¤§å€¼
            X[col] = X[col].replace([np.inf, -np.inf], np.nan)
            # ç»Ÿè®¡å¹¶å¤„ç†NaNå€¼
            nan_count_before = X[col].isna().sum()
            if nan_count_before > 0:
                # ç”¨ä¸­ä½æ•°å¡«å……NaNï¼ˆä¿ç•™æ‰€æœ‰æ ·æœ¬ï¼‰
                median_val = X[col].median()
                X[col] = X[col].fillna(median_val)
                print(f"   ç‰¹å¾ '{col}': å¡«å……äº† {nan_count_before} ä¸ªNaNå€¼ (ä½¿ç”¨ä¸­ä½æ•°: {median_val:.4f})")

        X = X.values  # è½¬æ¢å›numpyæ•°ç»„
        print("âœ… æ•°æ®æ¸…æ´—å®Œæˆ")
        # ============ æ•°æ®æ¸…æ´—ç»“æŸ ============

        # ç¼–ç æ ‡ç­¾
        self.label_encoder.fit(y)
        y_encoded = self.label_encoder.transform(y)

        print(f"\nğŸ“Š æ•°æ®ä¿¡æ¯:")
        print(f"   ç‰¹å¾å½¢çŠ¶: {X.shape}")
        print(f"   ç±»åˆ«æ•°: {len(self.label_encoder.classes_)}")
        print(f"   ç±»åˆ«ç¼–ç :")
        for i, cls in enumerate(self.label_encoder.classes_):
            print(f"     {i} -> Length {cls}")

        # æ ‡å‡†åŒ–ç‰¹å¾
        X_scaled = self.scaler.fit_transform(X)

        # åˆ’åˆ†è®­ç»ƒæµ‹è¯•é›†
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y_encoded,
            test_size=self.config['test_size'],
            random_state=42,
            stratify=y_encoded
        )

        print(f"\nğŸ“ˆ æ•°æ®åˆ†å‰²:")
        print(f"   è®­ç»ƒé›†: {X_train.shape[0]:,} æ ·æœ¬")
        print(f"   æµ‹è¯•é›†: {X_test.shape[0]:,} æ ·æœ¬")

        print(f"\nğŸ¯ è®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ:")
        unique, counts = np.unique(y_train, return_counts=True)
        for label, count in zip(unique, counts):
            label_name = self.label_encoder.inverse_transform([label])[0]
            percentage = count / len(y_train) * 100
            print(f"   Length {label_name}: {count:,}æ ·æœ¬ ({percentage:.1f}%)")

        # æ£€æŸ¥ç±»åˆ«ä¸å¹³è¡¡
        max_count = counts.max()
        min_count = counts.min()
        imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')
        if imbalance_ratio > 3:  # å¦‚æœæœ€å¤§ç±»æ¯”æœ€å°ç±»å¤š3å€ä»¥ä¸Š
            print(f"âš ï¸  æ£€æµ‹åˆ°ç±»åˆ«ä¸å¹³è¡¡: æœ€å¤§/æœ€å°æ ·æœ¬æ¯” = {imbalance_ratio:.1f}:1")
            print("   å°†åº”ç”¨ç±»åˆ«æƒé‡è¿›è¡Œå¹³è¡¡...")

        self.X_train, self.X_test = X_train, X_test
        self.y_train, self.y_test = y_train, y_test
        self.X = X_scaled
        self.y = y_encoded

        return X_train, X_test, y_train, y_test

    def select_important_features(self, importance_threshold=0.05):
        """åŸºäºç‰¹å¾é‡è¦æ€§é€‰æ‹©é‡è¦ç‰¹å¾"""
        print("\nğŸ” åŸºäºé‡è¦æ€§ç­›é€‰ç‰¹å¾...")

        # æ ¹æ®é‡è¦æ€§æŠ¥å‘Šï¼Œæ‰‹åŠ¨é€‰æ‹©é‡è¦ç‰¹å¾
        important_features = [
            'ECD_per_Dwell',
            'Dwell Time (s)_ratio_to_400',
            'Amplitude (pA)_ratio_to_400',
            'ECD (pC)_ratio_to_400',
            'size',
            'Amp_per_size',
            'ECD (pC)',
            'Dwell Time (s)',
            'Amplitude (pA)',
            'log_ECD',
            'log_Dwell Time'
        ]

        # æ£€æŸ¥è¿™äº›ç‰¹å¾æ˜¯å¦å­˜åœ¨äºå½“å‰ç‰¹å¾åˆ—è¡¨ä¸­
        current_features = set(self.feature_names)
        selected = [f for f in important_features if f in current_features]

        # æ·»åŠ å¯èƒ½é—æ¼çš„é‡è¦åŸå§‹ç‰¹å¾
        for feature in ['Dwell Time (s)', 'Amplitude (pA)', 'ECD (pC)', 'size']:
            if feature not in selected and feature in current_features:
                selected.append(feature)

        removed = len(self.feature_names) - len(selected)
        self.feature_names = selected

        print(f"âœ… ç‰¹å¾é€‰æ‹©å®Œæˆ: ä»{len(self.feature_names) + removed}ä¸ªç‰¹å¾ä¸­é€‰æ‹©{len(self.feature_names)}ä¸ª")
        print(f"   ç§»é™¤äº† {removed} ä¸ªä¸é‡è¦çš„ç‰¹å¾")
        print(f"   ä¿ç•™çš„ç‰¹å¾: {self.feature_names}")

        return self.feature_names

    def build_model(self):
        """æ„å»ºä¼˜åŒ–çš„ç¥ç»ç½‘ç»œæ¨¡å‹"""
        print("\n" + "=" * 80)
        print("æ„å»ºä¼˜åŒ–ç¥ç»ç½‘ç»œæ¨¡å‹")
        print("=" * 80)

        input_dim = len(self.feature_names)
        n_classes = len(self.label_encoder.classes_)

        model = models.Sequential()
        model.add(layers.Input(shape=(input_dim,)))

        # é’ˆå¯¹æ›´å¤šç‰¹å¾å¢åŠ ç½‘ç»œå®¹é‡
        model.add(layers.Dense(128, activation='relu',
                               kernel_regularizer=regularizers.l2(0.001)))
        model.add(layers.BatchNormalization())
        model.add(layers.Dropout(0.4))

        model.add(layers.Dense(64, activation='relu',
                               kernel_regularizer=regularizers.l2(0.001)))
        model.add(layers.BatchNormalization())
        model.add(layers.Dropout(0.3))

        model.add(layers.Dense(32, activation='relu'))
        model.add(layers.Dropout(0.2))

        # è¾“å‡ºå±‚
        model.add(layers.Dense(n_classes, activation='softmax'))

        # ä½¿ç”¨æ›´å°çš„åˆå§‹å­¦ä¹ ç‡
        optimizer = keras.optimizers.Adam(learning_rate=0.0005)

        # æ·»åŠ æ›´å¤šè¯„ä¼°æŒ‡æ ‡
        model.compile(optimizer=optimizer,
                      loss='sparse_categorical_crossentropy',
                      metrics=['accuracy',
                               keras.metrics.SparseTopKCategoricalAccuracy(k=2, name='top2_accuracy'),
                               keras.metrics.SparseCategoricalCrossentropy(name='xentropy')])

        print("ğŸ§  ä¼˜åŒ–æ¨¡å‹æ¶æ„:")
        model.summary()
        self.model = model
        return model

    def train_ensemble_model(self):
        """è®­ç»ƒç®€å•çš„é›†æˆå­¦ä¹ æ¨¡å‹ï¼ˆå¿«é€Ÿå®ç°ï¼‰"""
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.metrics import accuracy_score

        print("\n" + "=" * 80)
        print("ğŸŒ² è®­ç»ƒéšæœºæ£®æ—é›†æˆæ¨¡å‹")
        print("=" * 80)

        # ä½¿ç”¨ç›¸åŒçš„è®­ç»ƒæ•°æ®
        X_train, X_test, y_train, y_test = self.X_train, self.X_test, self.y_train, self.y_test

        # è®­ç»ƒéšæœºæ£®æ—
        rf_model = RandomForestClassifier(
            n_estimators=200,
            max_depth=15,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1,
            class_weight='balanced'
        )

        print("æ­£åœ¨è®­ç»ƒéšæœºæ£®æ—...")
        rf_model.fit(X_train, y_train)

        # è¯„ä¼°
        y_pred = rf_model.predict(X_test)
        rf_accuracy = accuracy_score(y_test, y_pred)

        print(f"ğŸ“Š éšæœºæ£®æ—æ€§èƒ½:")
        print(f"   æµ‹è¯•å‡†ç¡®ç‡: {rf_accuracy:.4f}")

        # å¦‚æœå·²ç»è®­ç»ƒè¿‡ç¥ç»ç½‘ç»œï¼Œåˆ™æ¯”è¾ƒ
        if hasattr(self, 'history') and self.history:
            nn_val_accuracy = max(self.history.history['val_accuracy'])
            print(f"   vs ç¥ç»ç½‘ç»œæœ€ä½³éªŒè¯å‡†ç¡®ç‡: {nn_val_accuracy:.4f}")

        # ä¿å­˜æ¨¡å‹
        output_dir = os.path.join(self.config['output_folder'], "ensemble")
        os.makedirs(output_dir, exist_ok=True)

        joblib.dump(rf_model, os.path.join(output_dir, "random_forest_model.pkl"))

        # ç‰¹å¾é‡è¦æ€§
        rf_importance = pd.DataFrame({
            'feature': self.feature_names,
            'importance': rf_model.feature_importances_
        }).sort_values('importance', ascending=False)

        print(f"\nğŸŒ³ éšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§:")
        print(rf_importance.head(10).to_string(index=False))

        # ä¿å­˜ç‰¹å¾é‡è¦æ€§
        rf_importance.to_csv(os.path.join(output_dir, "rf_feature_importance.csv"), index=False)

        # ä¸ç¥ç»ç½‘ç»œæ¯”è¾ƒ
        if hasattr(self, 'history') and self.history:
            nn_val_accuracy = max(self.history.history['val_accuracy'])
            if rf_accuracy > nn_val_accuracy:
                improvement = (rf_accuracy - nn_val_accuracy) * 100
                print(f"\nğŸ¯ éšæœºæ£®æ—æ¯”ç¥ç»ç½‘ç»œæœ€ä½³éªŒè¯å‡†ç¡®ç‡é«˜ {improvement:.1f}%")
                print("   å»ºè®®ä½¿ç”¨éšæœºæ£®æ—ä½œä¸ºä¸»è¦æ¨¡å‹")
            else:
                print(f"\nğŸ¯ ç¥ç»ç½‘ç»œä»ç„¶æ˜¯æœ€ä½³æ¨¡å‹")

        self.ensemble_model = rf_model
        self.rf_accuracy = rf_accuracy

        return rf_accuracy
    def train_model(self):
        """è®­ç»ƒæ¨¡å‹ï¼ˆå·²åŠ å…¥ç±»åˆ«å¹³è¡¡ï¼‰"""
        print("\n" + "=" * 80)
        print("è®­ç»ƒç¥ç»ç½‘ç»œ")
        print("=" * 80)

        output_dir = os.path.join(self.config['output_folder'], "training")
        os.makedirs(output_dir, exist_ok=True)

        # ============ ç±»åˆ«å¹³è¡¡ï¼šè®¡ç®—ç±»åˆ«æƒé‡ ============
        print("\nâš–ï¸  è®¡ç®—ç±»åˆ«æƒé‡ä»¥å¹³è¡¡æ•°æ®...")
        from sklearn.utils import compute_class_weight
        class_weights = compute_class_weight(
            'balanced',
            classes=np.unique(self.y_train),
            y=self.y_train
        )
        class_weight_dict = dict(enumerate(class_weights))

        print("ç±»åˆ«æƒé‡:")
        for class_idx, weight in class_weight_dict.items():
            class_name = self.label_encoder.inverse_transform([class_idx])[0]
            print(f"   Length {class_name}: æƒé‡ = {weight:.3f}")
        # ============ ç±»åˆ«å¹³è¡¡ç»“æŸ ============

        # å›è°ƒå‡½æ•°
        callbacks = [
            keras.callbacks.EarlyStopping(
                monitor='val_loss',
                patience=15,
                restore_best_weights=True,
                verbose=1
            ),
            keras.callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=7,
                min_lr=1e-6,
                verbose=1
            ),
            keras.callbacks.ModelCheckpoint(
                filepath=os.path.join(output_dir, 'best_model.h5'),
                monitor='val_accuracy',
                save_best_only=True,
                verbose=1
            )
        ]

        print(f"\nâš™ï¸ è®­ç»ƒå‚æ•°:")
        print(f"   è½®æ•°: {self.config['epochs']}")
        print(f"   æ‰¹æ¬¡: {self.config['batch_size']}")
        print(f"   éªŒè¯æ¯”ä¾‹: 10%")

        # è®­ç»ƒï¼ˆåº”ç”¨ç±»åˆ«æƒé‡ï¼‰
        self.history = self.model.fit(
            self.X_train, self.y_train,
            epochs=self.config['epochs'],
            batch_size=self.config['batch_size'],
            validation_split=0.1,
            callbacks=callbacks,
            class_weight=class_weight_dict,  # åº”ç”¨ç±»åˆ«æƒé‡
            verbose=1
        )

        # ç»˜åˆ¶è®­ç»ƒå†å²
        self._plot_training_history(output_dir)

        print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
        print(f"   æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {os.path.join(output_dir, 'best_model.h5')}")

        return self.history

    def _plot_training_history(self, output_dir):
        """ç»˜åˆ¶è®­ç»ƒå†å²"""
        history = self.history.history

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

        # æŸå¤±æ›²çº¿
        ax1.plot(history['loss'], label='è®­ç»ƒæŸå¤±', linewidth=2)
        if 'val_loss' in history:
            ax1.plot(history['val_loss'], label='éªŒè¯æŸå¤±', linewidth=2)
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.set_title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±')
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # å‡†ç¡®ç‡æ›²çº¿
        ax2.plot(history['accuracy'], label='è®­ç»ƒå‡†ç¡®ç‡', linewidth=2)
        if 'val_accuracy' in history:
            ax2.plot(history['val_accuracy'], label='éªŒè¯å‡†ç¡®ç‡', linewidth=2)
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy')
        ax2.set_title('è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡')
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'training_history.png'), dpi=300)
        plt.show(block=False)
        plt.pause(2)
        plt.close()

        # ä¿å­˜å†å²æ•°æ®
        pd.DataFrame(history).to_csv(os.path.join(output_dir, 'training_history.csv'), index=False)

        best_val_acc = max(history['val_accuracy']) if 'val_accuracy' in history else None
        if best_val_acc:
            print(f"   æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}")

    def evaluate_model(self):
        """è¯„ä¼°æ¨¡å‹"""
        print("\n" + "=" * 80)
        print("æ¨¡å‹è¯„ä¼°")
        print("=" * 80)

        output_dir = os.path.join(self.config['output_folder'], "evaluation")
        os.makedirs(output_dir, exist_ok=True)

        # è¯„ä¼°
        # ä¿®æ”¹ç¬¬812è¡Œ
        evaluation_results = self.model.evaluate(self.X_test, self.y_test, verbose=0)
        test_loss, test_accuracy = evaluation_results[0], evaluation_results[1]
        print(f"ğŸ“Š æµ‹è¯•é›†æ€§èƒ½:")
        print(f"   æŸå¤±: {test_loss:.4f}")
        print(f"   å‡†ç¡®ç‡: {test_accuracy:.4f}")

        # é¢„æµ‹
        y_pred_proba = self.model.predict(self.X_test, verbose=0)
        y_pred = np.argmax(y_pred_proba, axis=1) if len(self.label_encoder.classes_) > 2 else (
                y_pred_proba > 0.5).astype(int).flatten()

        # è§£ç 
        y_test_decoded = self.label_encoder.inverse_transform(self.y_test)
        y_pred_decoded = self.label_encoder.inverse_transform(y_pred)

        # åˆ†ç±»æŠ¥å‘Š
        print(f"\nğŸ“‹ åˆ†ç±»æŠ¥å‘Š:")
        report = classification_report(y_test_decoded, y_pred_decoded,
                                       target_names=[str(c) for c in self.label_encoder.classes_],
                                       output_dict=True)
        report_df = pd.DataFrame(report).transpose()
        print(report_df.round(4))
        report_df.to_csv(os.path.join(output_dir, 'classification_report.csv'))

        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(y_test_decoded, y_pred_decoded,
                              labels=self.label_encoder.classes_)

        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=self.label_encoder.classes_,
                    yticklabels=self.label_encoder.classes_)
        plt.xlabel('é¢„æµ‹æ ‡ç­¾')
        plt.ylabel('çœŸå®æ ‡ç­¾')
        plt.title('æ··æ·†çŸ©é˜µ')
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'confusion_matrix.png'), dpi=300)
        plt.show(block=False)
        plt.pause(2)
        plt.close()

        # ä¿å­˜æ··æ·†çŸ©é˜µ
        cm_df = pd.DataFrame(cm,
                             index=self.label_encoder.classes_,
                             columns=self.label_encoder.classes_)
        cm_df.to_csv(os.path.join(output_dir, 'confusion_matrix.csv'))

        # ç‰¹å¾é‡è¦æ€§
        self._analyze_feature_importance(output_dir)

        # ä¿å­˜æ¨¡å‹å’Œå·¥å…·
        self._save_model_and_tools(output_dir)

        return test_accuracy

    def _analyze_feature_importance(self, output_dir):
        """åˆ†æç‰¹å¾é‡è¦æ€§"""
        print(f"\nğŸ” ç‰¹å¾é‡è¦æ€§åˆ†æ:")

        if len(self.model.layers) > 0:
            # åŸºäºç¬¬ä¸€å±‚æƒé‡
            weights = self.model.layers[0].get_weights()[0]
            importance = np.mean(np.abs(weights), axis=1)

            importance_df = pd.DataFrame({
                'ç‰¹å¾': self.feature_names,
                'é‡è¦æ€§': importance
            }).sort_values('é‡è¦æ€§', ascending=False)

            print(importance_df.to_string(index=False))

            # å¯è§†åŒ–
            plt.figure(figsize=(10, 5))
            bars = plt.barh(range(len(importance_df)), importance_df['é‡è¦æ€§'])
            plt.yticks(range(len(importance_df)), importance_df['ç‰¹å¾'])
            plt.xlabel('é‡è¦æ€§å¾—åˆ†')
            plt.title('ç‰¹å¾é‡è¦æ€§')
            plt.gca().invert_yaxis()

            for bar, score in zip(bars, importance_df['é‡è¦æ€§']):
                plt.text(bar.get_width() + 0.001, bar.get_y() + bar.get_height() / 2,
                         f'{score:.3f}', va='center')

            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, 'feature_importance.png'), dpi=300)
            plt.show(block=False)
            plt.pause(2)
            plt.close()

            importance_df.to_csv(os.path.join(output_dir, 'feature_importance.csv'), index=False)

    def _save_model_and_tools(self, output_dir):
        """ä¿å­˜æ¨¡å‹å’Œå·¥å…·"""
        # ä¿å­˜å®Œæ•´æ¨¡å‹
        model_path = os.path.join(output_dir, 'final_model.h5')
        self.model.save(model_path)

        # ä¿å­˜é¢„å¤„ç†å·¥å…·
        joblib.dump(self.scaler, os.path.join(output_dir, 'scaler.pkl'))
        joblib.dump(self.label_encoder, os.path.join(output_dir, 'label_encoder.pkl'))

        # ä¿å­˜é…ç½®
        pd.DataFrame([self.config]).to_csv(os.path.join(output_dir, 'training_config.csv'), index=False)

        print(f"\nğŸ’¾ æ¨¡å‹å’Œå·¥å…·å·²ä¿å­˜:")
        print(f"   æ¨¡å‹: {model_path}")
        print(f"   æ ‡å‡†åŒ–å™¨: {os.path.join(output_dir, 'scaler.pkl')}")
        print(f"   æ ‡ç­¾ç¼–ç å™¨: {os.path.join(output_dir, 'label_encoder.pkl')}")

    def analyze_intra_pore_features(self):
        """åˆ†æåŒä¸€å­”å†…400bpä¸å…¶ä»–é•¿åº¦çš„ç‰¹å¾å…³ç³»"""
        print("\n" + "=" * 80)
        print("åŒä¸€å­”å†…ç‰¹å¾å¯¹æ¯”åˆ†æ: 400bp vs å…¶ä»–é•¿åº¦")
        print("=" * 80)

        if self.data is None:
            print("é”™è¯¯: è¯·å…ˆåŠ è½½æ•°æ® (è°ƒç”¨ load_and_preprocess_data)")
            return

        # æ£€æŸ¥å¿…è¦çš„åˆ—
        pore_column = self.config.get('pore_column', 'Pore Number')
        required_cols = [pore_column, 'Length'] + self.feature_names
        missing = [col for col in required_cols if col not in self.data.columns]
        if missing:
            print(f"é”™è¯¯: æ•°æ®ç¼ºå°‘ä»¥ä¸‹åˆ—: {missing}")
            return
        output_dir = os.path.join(self.config['output_folder'], "intra_pore_analysis")

        # ç¡®ä¿ç›®å½•å­˜åœ¨ï¼ˆæ›´ç¨³å¥çš„åˆ›å»ºæ–¹å¼ï¼‰
        try:
            os.makedirs(output_dir, exist_ok=True)
            print(f"âœ… åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
        except Exception as e:
            print(f"âŒ åˆ›å»ºç›®å½•å¤±è´¥: {e}")
            # å°è¯•åˆ›å»ºçˆ¶ç›®å½•
            parent_dir = os.path.dirname(output_dir)
            try:
                os.makedirs(parent_dir, exist_ok=True)
                os.makedirs(output_dir, exist_ok=True)
                print(f"âœ… é‡æ–°åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
            except Exception as e2:
                print(f"âŒ æœ€ç»ˆåˆ›å»ºç›®å½•å¤±è´¥ï¼Œä½¿ç”¨ä¸´æ—¶ç›®å½•")
                output_dir = os.path.join(os.getcwd(), "temp_intra_pore_analysis")
                os.makedirs(output_dir, exist_ok=True)

        print(f"ğŸ“ è¾“å‡ºç›®å½•ç¡®è®¤: {output_dir}")
        # ============ ç›®å½•åˆ›å»ºç»“æŸ ============

        # 2. ç­›é€‰å‡ºåŒ…å«400bpäº‹ä»¶çš„å­”
        pores_with_400 = self.data[self.data['Length'] == 400][pore_column].unique()
        analysis_data = self.data[self.data[pore_column].isin(pores_with_400)].copy()
        # ç­›é€‰å‡ºåŒ…å«400bpäº‹ä»¶çš„å­”
        pores_with_400 = self.data[self.data['Length'] == 400][pore_column].unique()
        analysis_data = self.data[self.data[pore_column].isin(pores_with_400)].copy()

        print(f"ğŸ“ˆ æ•°æ®æ¦‚è§ˆ:")
        print(f"   æ€»å­”æ•°: {self.data[pore_column].nunique()}")
        print(f"   åŒ…å«400bpäº‹ä»¶çš„å­”æ•°: {len(pores_with_400)}")
        print(f"   ç”¨äºåˆ†æçš„äº‹ä»¶æ•°: {len(analysis_data):,}")

        # æŒ‰å­”ç»Ÿè®¡äº‹ä»¶æ•°ï¼Œè¯†åˆ«æœ‰æ•ˆå­”
        pore_stats = analysis_data.groupby(pore_column).agg({
            'Length': ['count', 'nunique']
        }).round(2)
        pore_stats.columns = ['æ€»äº‹ä»¶æ•°', 'ä¸åŒLengthæ•°']
        pore_stats = pore_stats.sort_values('æ€»äº‹ä»¶æ•°', ascending=False)

        # å®šä¹‰æœ‰æ•ˆå­”çš„æ ‡å‡† (å¯è°ƒæ•´)
        MIN_EVENTS_PER_PORE = 10  # ä¸€ä¸ªå­”è‡³å°‘è¦æœ‰10ä¸ªäº‹ä»¶
        MIN_LENGTHS_PER_PORE = 2  # ä¸€ä¸ªå­”è‡³å°‘è¦æœ‰2ç§ä¸åŒçš„Length

        valid_pores = pore_stats[
            (pore_stats['æ€»äº‹ä»¶æ•°'] >= MIN_EVENTS_PER_PORE) &
            (pore_stats['ä¸åŒLengthæ•°'] >= MIN_LENGTHS_PER_PORE)
            ].index

        print(f"\nğŸ” å­”è¿‡æ»¤æ ‡å‡†: äº‹ä»¶æ•°â‰¥{MIN_EVENTS_PER_PORE}, ä¸åŒLengthæ•°â‰¥{MIN_LENGTHS_PER_PORE}")
        print(f"   è¿‡æ»¤å‰å­”æ•°: {len(pore_stats)}")
        print(f"   æœ‰æ•ˆå­”æ•°: {len(valid_pores)}")

        if len(valid_pores) == 0:
            print("è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„å­”ï¼Œæ­£åœ¨é™ä½æ ‡å‡†...")
            valid_pores = pore_stats[pore_stats['æ€»äº‹ä»¶æ•°'] >= 5].index
            print(f"   ä½¿ç”¨å®½æ¾æ ‡å‡†åçš„æœ‰æ•ˆå­”æ•°: {len(valid_pores)}")

        if len(valid_pores) == 0:
            print("é”™è¯¯: æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œå­”å†…åˆ†æ")
            return

        # ä½¿ç”¨æœ‰æ•ˆå­”çš„æ•°æ®
        valid_data = analysis_data[analysis_data[pore_column].isin(valid_pores)].copy()

        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = os.path.join(self.config['output_folder'], "intra_pore_analysis")
        os.makedirs(output_dir, exist_ok=True)

        # åˆ†ææ¯ä¸ªæœ‰æ•ˆå­”çš„ç‰¹å¾åˆ†å¸ƒ
        print(f"\nğŸ“Š æ­£åœ¨åˆ†æ {len(valid_pores)} ä¸ªæœ‰æ•ˆå­”çš„ç‰¹å¾åˆ†å¸ƒ...")

        all_pore_results = []

        for i, pore in enumerate(valid_pores[:20]):  # å…ˆåˆ†æå‰20ä¸ªå­”ä½œä¸ºç¤ºä¾‹
            pore_events = valid_data[valid_data[pore_column] == pore]

            # æ£€æŸ¥è¯¥å­”æ˜¯å¦æœ‰400bpå’Œå…¶ä»–é•¿åº¦
            lengths_in_pore = pore_events['Length'].unique()
            if 400 not in lengths_in_pore or len(lengths_in_pore) < 2:
                continue

            # æŒ‰é•¿åº¦åˆ†ç»„è®¡ç®—ç»Ÿè®¡é‡
            length_groups = pore_events.groupby('Length')

            pore_result = {pore_column: pore, 'Total Events': len(pore_events)}

            for length, group in length_groups:
                for feature in self.feature_names:
                    if feature in group.columns:
                        prefix = f"L{length}_{feature[:3]}"  # ä¾‹å¦‚: L400_Dwe, L500_Dwe
                        pore_result[f"{prefix}_mean"] = group[feature].mean()
                        pore_result[f"{prefix}_std"] = group[feature].std()
                        pore_result[f"{prefix}_median"] = group[feature].median()

            all_pore_results.append(pore_result)

            # ä¸ºæ¯ä¸ªå­”ç»˜åˆ¶ç‰¹å¾å¯¹æ¯”å›¾ (å¯é€‰ï¼Œå‰5ä¸ªå­”)
            if i < 5 and len(lengths_in_pore) >= 2:
                self._plot_pore_features(pore_events, pore, pore_column, output_dir)

        # æ±‡æ€»åˆ†æç»“æœ
        if all_pore_results:
            results_df = pd.DataFrame(all_pore_results)
            results_path = os.path.join(output_dir, "intra_pore_feature_summary.csv")
            results_df.to_csv(results_path, index=False, encoding='utf-8-sig')

            print(f"\nâœ… å­”å†…åˆ†æå®Œæˆ!")
            print(f"   åˆ†æäº† {len(results_df)} ä¸ªå­”çš„æ•°æ®")
            print(f"   è¯¦ç»†ç»“æœä¿å­˜è‡³: {results_path}")

            # é‡ç‚¹: è®¡ç®—400bpç›¸å¯¹äºå…¶ä»–é•¿åº¦çš„ç‰¹å¾å·®å¼‚
            self._calculate_400bp_relative_features(valid_data, pore_column, output_dir)

        return valid_data

    def _plot_pore_features(self, pore_data, pore_id, pore_column, output_dir):
        """ä¸ºå•ä¸ªå­”ç»˜åˆ¶ç‰¹å¾å¯¹æ¯”å›¾"""
        # è®¾ç½®å›¾å½¢
        features_to_plot = ['Dwell Time (s)', 'Amplitude (pA)', 'ECD (pC)']
        n_features = len(features_to_plot)

        fig, axes = plt.subplots(1, n_features, figsize=(5 * n_features, 6))
        if n_features == 1:
            axes = [axes]

        pore_data = pore_data.copy()

        # ä¸ºä¸åŒé•¿åº¦åˆ›å»ºé¢œè‰²æ˜ å°„
        unique_lengths = sorted(pore_data['Length'].unique())
        palette = sns.color_palette("husl", len(unique_lengths))
        length_to_color = dict(zip(unique_lengths, palette))

        for idx, feature in enumerate(features_to_plot):
            ax = axes[idx]

            # åˆ›å»ºç®±çº¿å›¾
            box_data = []
            box_labels = []
            for length in unique_lengths:
                length_data = pore_data[pore_data['Length'] == length][feature].dropna()
                if len(length_data) > 0:
                    box_data.append(length_data)
                    box_labels.append(str(length))

            if box_data:
                box_plot = ax.boxplot(box_data, labels=box_labels, patch_artist=True)

                # ä¸ºæ¯ä¸ªç®±ä½“ä¸Šè‰²
                for i, (patch, length) in enumerate(zip(box_plot['boxes'], unique_lengths)):
                    if length in length_to_color:
                        patch.set_facecolor(length_to_color[length])
                        patch.set_alpha(0.7)

                # çªå‡ºæ˜¾ç¤º400bp
                if 400 in unique_lengths:
                    idx_400 = list(unique_lengths).index(400)
                    if idx_400 < len(box_plot['boxes']):
                        box_plot['boxes'][idx_400].set_edgecolor('red')
                        box_plot['boxes'][idx_400].set_linewidth(2)

            ax.set_title(f'Pore {pore_id}: {feature}')
            ax.set_xlabel('Length (bp)')
            ax.set_ylabel(feature)
            ax.grid(True, alpha=0.3)

        plt.suptitle(f'Pore {pore_id} - ä¸åŒLengthçš„ç‰¹å¾åˆ†å¸ƒå¯¹æ¯”', fontsize=14)
        plt.tight_layout()

        # ä¿å­˜å›¾åƒ
        plot_path = os.path.join(output_dir, f"pore_{pore_id}_feature_comparison.png")
        plt.savefig(plot_path, dpi=150, bbox_inches='tight')
        plt.close()

        # é¢å¤–: ç»˜åˆ¶æ•£ç‚¹çŸ©é˜µ
        if len(pore_data) >= 10:
            fig = sns.pairplot(pore_data,
                               hue='Length',
                               vars=features_to_plot,
                               palette=length_to_color,
                               plot_kws={'alpha': 0.6, 's': 30})
            fig_path = os.path.join(output_dir, f"pore_{pore_id}_pairplot.png")
            fig.savefig(fig_path, dpi=150, bbox_inches='tight')
            plt.close()

    def _calculate_400bp_relative_features(self, data, pore_column, output_dir):
        """è®¡ç®—400bpç›¸å¯¹äºå…¶ä»–é•¿åº¦çš„ç‰¹å¾å·®å¼‚"""
        print(f"\nğŸ”¬ è®¡ç®—400bpçš„ç›¸å¯¹ç‰¹å¾...")

        results = []

        # æŒ‰å­”åˆ†ç»„å¤„ç†
        for pore, pore_group in data.groupby(pore_column):
            # æ£€æŸ¥è¯¥å­”æ˜¯å¦æœ‰400bpäº‹ä»¶
            events_400 = pore_group[pore_group['Length'] == 400]
            other_events = pore_group[pore_group['Length'] != 400]

            if len(events_400) == 0 or len(other_events) == 0:
                continue

            # è®¡ç®—400bpçš„ç‰¹å¾å¹³å‡å€¼
            mean_400 = events_400[self.feature_names].mean()

            # è®¡ç®—å…¶ä»–é•¿åº¦çš„å¹³å‡å€¼
            mean_other = other_events[self.feature_names].mean()

            # è®¡ç®—æ¯”å€¼ (400bp / å…¶ä»–)
            ratio = mean_400 / mean_other.replace(0, np.nan)  # é¿å…é™¤é›¶

            # è®¡ç®—å·®å€¼
            diff = mean_400 - mean_other

            # è®¡ç®—æ ‡å‡†åŒ–å·®å€¼ (Z-score)
            std_other = other_events[self.feature_names].std()
            z_diff = diff / std_other.replace(0, np.nan)

            # ä¿å­˜ç»“æœ
            pore_result = {
                'Pore_Number': pore,
                'Events_400bp': len(events_400),
                'Events_Other': len(other_events),
                'Other_Lengths': ','.join(map(str, other_events['Length'].unique()))
            }

            for feature in self.feature_names:
                pore_result[f'{feature}_400_mean'] = mean_400[feature]
                pore_result[f'{feature}_other_mean'] = mean_other[feature]
                pore_result[f'{feature}_ratio'] = ratio[feature]
                pore_result[f'{feature}_diff'] = diff[feature]
                pore_result[f'{feature}_z_diff'] = z_diff[feature]

            results.append(pore_result)

        if results:
            rel_df = pd.DataFrame(results)

            # ä¿å­˜è¯¦ç»†ç»“æœ
            detail_path = os.path.join(output_dir, "400bp_relative_features_detailed.csv")
            rel_df.to_csv(detail_path, index=False, encoding='utf-8-sig')

            # ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡
            print(f"\nğŸ“ˆ 400bpç›¸å¯¹ç‰¹å¾æ±‡æ€» (åŸºäº {len(rel_df)} ä¸ªå­”):")

            # å¯¹æ¯ä¸ªç‰¹å¾ï¼Œç»Ÿè®¡æ¯”å€¼å’Œå·®å¼‚
            for feature in self.feature_names:
                ratio_col = f'{feature}_ratio'
                diff_col = f'{feature}_diff'

                if ratio_col in rel_df.columns:
                    ratios = rel_df[ratio_col].dropna()
                    if len(ratios) > 0:
                        print(f"\n  {feature}:")
                        print(f"    æ¯”å€¼(400/å…¶ä»–) - ä¸­ä½æ•°: {ratios.median():.3f}, å‡å€¼: {ratios.mean():.3f}")
                        print(f"    èŒƒå›´: [{ratios.min():.3f}, {ratios.max():.3f}]")

            # ä¿å­˜æ±‡æ€»ç»Ÿè®¡
            summary_cols = [col for col in rel_df.columns if any(x in col for x in ['_ratio', '_diff', '_z_diff'])]
            if summary_cols:
                summary = rel_df[summary_cols].describe().round(4)
                summary_path = os.path.join(output_dir, "400bp_relative_features_summary.csv")
                summary.to_csv(summary_path, encoding='utf-8-sig')

                print(f"\nğŸ“‹ æ±‡æ€»ç»Ÿè®¡å·²ä¿å­˜: {summary_path}")
                print(summary)

            print(f"\nğŸ’¡ å…³é”®æ´å¯Ÿå»ºè®®:")
            print("  1. å¦‚æœæŸä¸ªç‰¹å¾çš„æ¯”å€¼ç¨³å®šè¿œç¦»1.0ï¼Œå®ƒæ˜¯å¥½çš„åŒºåˆ†æŒ‡æ ‡")
            print("  2. Z-scoreå·®å¼‚è¶Šå¤§ï¼Œè¯¥ç‰¹å¾åœ¨å­”å†…çš„åŒºåˆ†åº¦è¶Šå¥½")
            print("  3. å¯ä»¥å°†è¿™äº›æ¯”å€¼/å·®å¼‚ä½œä¸ºæ–°ç‰¹å¾åŠ å…¥åˆ†ç±»æ¨¡å‹")

            # å¯è§†åŒ–æ¯”å€¼åˆ†å¸ƒ
            self._plot_relative_feature_distributions(rel_df, output_dir)

        return rel_df if 'rel_df' in locals() else None

    def _plot_relative_feature_distributions(self, rel_df, output_dir):
        """ç»˜åˆ¶ç›¸å¯¹ç‰¹å¾çš„åˆ†å¸ƒå›¾"""
        # ç­›é€‰æ¯”å€¼ç‰¹å¾
        ratio_cols = [col for col in rel_df.columns if '_ratio' in col]

        if not ratio_cols:
            return

        n_features = len(ratio_cols)
        fig, axes = plt.subplots(1, n_features, figsize=(5 * n_features, 5))

        if n_features == 1:
            axes = [axes]

        for idx, col in enumerate(ratio_cols):
            ax = axes[idx]
            feature_name = col.replace('_ratio', '')

            # ç»˜åˆ¶åˆ†å¸ƒç›´æ–¹å›¾
            ratios = rel_df[col].dropna()
            ax.hist(ratios, bins=30, alpha=0.7, color='skyblue', edgecolor='black')

            # æ·»åŠ å‚è€ƒçº¿ (æ¯”å€¼=1è¡¨ç¤ºæ— å·®å¼‚)
            ax.axvline(x=1.0, color='red', linestyle='--', linewidth=2, label='æ— å·®å¼‚çº¿(æ¯”å€¼=1)')

            # æ·»åŠ ä¸­ä½æ•°çº¿
            median_val = ratios.median()
            ax.axvline(x=median_val, color='green', linestyle='-', linewidth=2,
                       label=f'ä¸­ä½æ•°: {median_val:.2f}')

            ax.set_xlabel(f'{feature_name} æ¯”å€¼ (400bp/å…¶ä»–)')
            ax.set_ylabel('å­”çš„æ•°é‡')
            ax.set_title(f'{feature_name}: 400bpç›¸å¯¹æ¯”å€¼åˆ†å¸ƒ')
            ax.legend()
            ax.grid(True, alpha=0.3)

        plt.suptitle('400bpç›¸å¯¹å…¶ä»–é•¿åº¦çš„ç‰¹å¾æ¯”å€¼åˆ†å¸ƒ', fontsize=14)
        plt.tight_layout()

        plot_path = os.path.join(output_dir, "400bp_relative_ratios_distribution.png")
        plt.savefig(plot_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"ğŸ“Š ç›¸å¯¹ç‰¹å¾åˆ†å¸ƒå›¾å·²ä¿å­˜: {plot_path}")

    def run_pipeline(self):
        """è¿è¡Œå®Œæ•´æµç¨‹"""
        try:
            print("\n" + "=" * 80)
            print("ğŸš€ å¼€å§‹çº³ç±³å­”Lengthåˆ†ç±»æµç¨‹")
            print("=" * 80)

            # 1. äº¤äº’å¼è®¾ç½®
            self.run_interactive_setup()

            # 2. åŠ è½½æ•°æ®
            self.load_and_preprocess_data()

            # 3. é€‰æ‹©Length
            self.select_target_lengths()

            # 4. æ¢ç´¢æ€§åˆ†æï¼ˆå¯é€‰ï¼‰
            if input("\nè¿›è¡Œæ¢ç´¢æ€§æ•°æ®åˆ†æ? (y/n) [y]: ").strip().lower() != 'n':
                self.explore_data()

            # 5. ECDå¼‚å¸¸å€¼åˆ†æï¼ˆå…³é”®æ­¥éª¤ï¼‰
            print("\n" + "=" * 80)
            print("âš ï¸  ECDå¼‚å¸¸å€¼å¤„ç†ï¼ˆå¼ºçƒˆæ¨èï¼‰")
            print("=" * 80)
            print("æ‚¨çš„æ•°æ®æ˜¾ç¤ºECDå­˜åœ¨æç«¯å¼‚å¸¸å€¼ï¼Œå¯èƒ½ä¸¥é‡å½±å“æ¨¡å‹æ€§èƒ½ã€‚")

            if input("è¿›è¡ŒECDå¼‚å¸¸å€¼åˆ†æ? (y/n) [y]: ").strip().lower() != 'n':
                self.analyze_ecd_outliers()
            else:
                print("å·²è·³è¿‡å¼‚å¸¸å€¼åˆ†æã€‚")

            # 6. åŒä¸€å­”å†…ç‰¹å¾å¯¹æ¯”åˆ†æï¼ˆæ–°å¢æ­¥éª¤ï¼‰
            print("\n" + "=" * 80)
            print("ğŸ”¬ åŒä¸€å­”å†…ç‰¹å¾å¯¹æ¯”åˆ†æ")
            print("=" * 80)
            print("åˆ†æåŒä¸€çº³ç±³å­”å†…400bpä¸å…¶ä»–é•¿åº¦çš„ç‰¹å¾å…³ç³»ï¼Œç”¨äºå‘ç°æ›´å¥½çš„åŒºåˆ†ç‰¹å¾ã€‚")

            if input("è¿›è¡ŒåŒä¸€å­”å†…ç‰¹å¾å¯¹æ¯”åˆ†æ? (y/n) [y]: ").strip().lower() != 'n':
                self.analyze_intra_pore_features()
                input("\næŒ‰Enteré”®ç»§ç»­è®­ç»ƒæµç¨‹...")

            # 7. å‡†å¤‡è®­ç»ƒæ•°æ®
            self.prepare_training_data()

            # 8. æ„å»ºæ¨¡å‹
            self.build_model()

            # 9. è®­ç»ƒæ¨¡å‹
            self.train_model()

            # 10. è¯„ä¼°æ¨¡å‹
            accuracy = self.evaluate_model()

            # 11. äº¤å‰éªŒè¯ï¼ˆå¯é€‰ï¼‰
            if input("\nè¿›è¡Œäº¤å‰éªŒè¯? (y/n) [n]: ").strip().lower() == 'y':
                self._run_cross_validation()

            # 12. é¢„æµ‹æ–°æ•°æ®ï¼ˆå¯é€‰ï¼‰
            if input("\né¢„æµ‹æ–°æ•°æ®? (y/n) [n]: ").strip().lower() == 'y':
                self._predict_new_data()

            print("\n" + "=" * 80)
            print("ğŸ‰ æµç¨‹å®Œæˆ!")
            print("=" * 80)
            print(f"æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {accuracy:.4f}")
            print(f"\næ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {self.config['output_folder']}")
            print("\nğŸ“ è¾“å‡ºæ–‡ä»¶å¤¹ç»“æ„:")
            print("   exploration/        - æ¢ç´¢æ€§åˆ†æç»“æœ")
            print("   outlier_analysis/   - å¼‚å¸¸å€¼è¯¦ç»†æŠ¥å‘Š")
            print("   intra_pore_analysis/ - åŒä¸€å­”å†…ç‰¹å¾å¯¹æ¯”åˆ†æ")
            print("   training/          - è®­ç»ƒè¿‡ç¨‹å’Œæœ€ä½³æ¨¡å‹")
            print("   evaluation/        - æ¨¡å‹è¯„ä¼°ç»“æœ")
            print("\næ„Ÿè°¢ä½¿ç”¨çº³ç±³å­”Lengthåˆ†ç±»ç³»ç»Ÿ!")

        except Exception as e:
            print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {str(e)}")
            import traceback
            traceback.print_exc()

            if input("\næ˜¯å¦é‡æ–°è¿è¡Œ? (y/n) [n]: ").strip().lower() == 'y':
                self.__init__()
                self.run_pipeline()

    def _run_cross_validation(self):
        """äº¤å‰éªŒè¯ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        print("\nè¿›è¡Œäº¤å‰éªŒè¯...")
        # è¿™é‡Œå¯ä»¥æ·»åŠ äº¤å‰éªŒè¯ä»£ç 
        print("äº¤å‰éªŒè¯åŠŸèƒ½å¾…å®ç°")

    def _predict_new_data(self):
        """é¢„æµ‹æ–°æ•°æ®ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        print("\né¢„æµ‹æ–°æ•°æ®...")
        # è¿™é‡Œå¯ä»¥æ·»åŠ é¢„æµ‹æ–°æ•°æ®çš„ä»£ç 
        print("æ–°æ•°æ®é¢„æµ‹åŠŸèƒ½å¾…å®ç°")


# ============ ä¸»ç¨‹åº ============
def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("ğŸ§¬ çº³ç±³å­”Lengthåˆ†ç±»ç³»ç»Ÿ v2.2")
    print("=" * 80)
    print("åŸºäºå››ä¸ªå‚æ•°åŒºåˆ†Length:")
    print("  1. Dwell Time (s)")
    print("  2. Amplitude (pA)")
    print("  3. ECD (pC)")
    print("  4. size")
    print("\nâœ… å·²é›†æˆä»¥ä¸‹æ”¹è¿›:")
    print("  â€¢ è‡ªåŠ¨æ•°æ®æ¸…æ´—ï¼ˆå¤„ç†NaN/Infå€¼ï¼‰")
    print("  â€¢ ç±»åˆ«å¹³è¡¡ï¼ˆè‡ªåŠ¨è®¡ç®—ç±»åˆ«æƒé‡ï¼‰")
    print("  â€¢ åŒä¸€å­”å†…ç‰¹å¾å¯¹æ¯”åˆ†æï¼ˆæ–°å¢ï¼‰")
    print("=" * 80)

    # åˆ›å»ºåˆ†ç±»å™¨å¹¶è¿è¡Œ
    classifier = NanoPoreLengthClassifier()
    classifier.run_pipeline()


if __name__ == "__main__":
    main()